@article{auer2002,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicolo and Fischer, Paul},
  year = {2002},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2DIANKMA/Auer et al. - Finite-time Analysis of the Multiarmed Bandit Problem.pdf}
}

@article{browne2012,
  title = {A Survey of {{Monte Carlo}} Tree Search Methods},
  author = {Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  year = {2012},
  month = mar,
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  volume = {4},
  number = {1},
  pages = {1--43},
  issn = {1943068X},
  doi = {10.1109/TCIAIG.2012.2186810},
  urldate = {2023-03-22},
  abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work. {\copyright} 2009 IEEE.},
  keywords = {Artificial intelligence (AI),bandit-based methods,computer Go,game search,Monte Carlo tree search (MCTS),upper confidence bounds (UCB),upper confidence bounds for trees (UCT)},
  file = {/Users/nobr/Zotero/storage/JALIHS9D/Browne et al. - 2012 - A survey of Monte Carlo tree search methods.pdf}
}

@misc{cloos2024,
  title = {Baba {{Is AI}}: {{Break}} the {{Rules}} to {{Beat}} the {{Benchmark}}},
  shorttitle = {Baba {{Is AI}}},
  author = {Cloos, Nathan and Jens, Meagan and Naim, Michelangelo and Kuo, Yen-Ling and Cases, Ignacio and Barbu, Andrei and Cueva, Christopher J.},
  year = {2024},
  month = jul,
  number = {arXiv:2407.13729},
  eprint = {2407.13729},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.13729},
  urldate = {2025-08-09},
  abstract = {Humans solve problems by following existing rules and procedures, and also by leaps of creativity to redefine those rules and objectives. To probe these abilities, we developed a new benchmark based on the game Baba Is You where an agent manipulates both objects in the environment and rules, represented by movable tiles with words written on them, to reach a specified goal and win the game. We test three state-of-the-art multimodal large language models (OpenAI GPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail dramatically when generalization requires that the rules of the game must be manipulated and combined.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/4DSAUUAZ/Cloos et al. - 2024 - Baba Is AI Break the Rules to Beat the Benchmark.pdf}
}

@inproceedings{dehghani2022,
  title = {{{SCENIC}}: {{A JAX Library}} for {{Computer Vision Research}} and {{Beyond}}},
  shorttitle = {{{SCENIC}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Dehghani, Mostafa and Gritsenko, Alexey and Arnab, Anurag and Minderer, Matthias and Tay, Yi},
  year = {2022},
  month = jun,
  pages = {21361--21366},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.02070},
  urldate = {2025-08-07},
  abstract = {SCENIC is an open-source1 JAX library with a focus on transformer-based models for computer vision research and beyond. The goal of this toolkit is to facilitate rapid experimentation, prototyping, and research of new architectures and models. SCENIC supports a diverse range of tasks (e.g., classification, segmentation, detection) and facilitates working on multi-modal problems, along with GPU/TPU support for large-scale, multi-host and multi-device training. SCENIC also offers optimized implementations of stateof-the-art research models spanning a wide range of modalities. SCENIC has been successfully used for numerous projects and published papers and continues serving as the library of choice for rapid prototyping and publication of new research ideas.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/PB58GHEH/Dehghani et al. - 2022 - SCENIC A JAX Library for Computer Vision Research and Beyond.pdf}
}

@misc{hu2024c,
  title = {Games for {{Artificial Intelligence Research}}: {{A Review}} and {{Perspectives}}},
  shorttitle = {Games for {{Artificial Intelligence Research}}},
  author = {Hu, Chengpeng and Zhao, Yunlong and Wang, Ziqi and Du, Haocheng and Liu, Jialin},
  year = {2024},
  month = jun,
  number = {arXiv:2304.13269},
  eprint = {2304.13269},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.13269},
  urldate = {2025-08-08},
  abstract = {Games have been the perfect test-beds for artificial intelligence research for the characteristics that widely exist in real-world scenarios. Learning and optimisation, decision making in dynamic and uncertain environments, game theory, planning and scheduling, design and education are common research areas shared between games and real-world problems. Numerous opensource games or game-based environments have been implemented for studying artificial intelligence. In addition to singleor multi-player, collaborative or adversarial games, there has also been growing interest in implementing platforms for creative design in recent years. Those platforms provide ideal benchmarks for exploring and comparing artificial intelligence ideas and techniques. This paper reviews the game-based platforms for artificial intelligence research, provides guidance on matching particular types of artificial intelligence with suitable games for testing and matching particular needs in games with suitable artificial intelligence techniques, discusses the research trend induced by the evolution of those platforms, and gives an outlook.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/2H9EVG4V/Hu et al. - 2024 - Games for Artificial Intelligence Research A Review and Perspectives.pdf}
}

@misc{khalifa2020,
  title = {{{PCGRL}}: {{Procedural Content Generation}} via {{Reinforcement Learning}}},
  shorttitle = {{{PCGRL}}},
  author = {Khalifa, Ahmed and Bontrager, Philip and Earle, Sam and Togelius, Julian},
  year = {2020},
  month = aug,
  number = {arXiv:2001.09212},
  eprint = {2001.09212},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.09212},
  urldate = {2025-08-09},
  abstract = {We investigate how reinforcement learning can be used to train level-designing agents. This represents a new approach to procedural content generation in games, where level design is framed as a game, and the content generator itself is learned. By seeing the design problem as a sequential task, we can use reinforcement learning to learn how to take the next action so that the expected final level quality is maximized. This approach can be used when few or no examples exist to train from, and the trained generator is very fast. We investigate three different ways of transforming two-dimensional level design problems into Markov decision processes and apply these to three game environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/5SG6YE2Q/Khalifa et al. - 2020 - PCGRL Procedural Content Generation via Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/QICUW9HJ/2001.html}
}

@misc{kumar2024,
  title = {Automating the {{Search}} for {{Artificial Life}} with {{Foundation Models}}},
  author = {Kumar, Akarsh and Lu, Chris and Kirsch, Louis and Tang, Yujin and Stanley, Kenneth O. and Isola, Phillip and Ha, David},
  year = {2024},
  month = dec,
  number = {arXiv:2412.17799},
  eprint = {2412.17799},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.17799},
  urldate = {2025-01-09},
  abstract = {With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/NSHHFFYA/Kumar et al. - 2024 - Automating the Search for Artificial Life with Foundation Models.pdf}
}

@misc{malagon2024,
  title = {Craftium: {{An Extensible Framework}} for {{Creating Reinforcement Learning Environments}}},
  shorttitle = {Craftium},
  author = {Malag{\'o}n, Mikel and Ceberio, Josu and Lozano, Jose A.},
  year = {2024},
  month = jul,
  number = {arXiv:2407.03969},
  eprint = {2407.03969},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.03969},
  urldate = {2024-12-17},
  abstract = {Most Reinforcement Learning (RL) environments are created by adapting existing physics simulators or video games. However, they usually lack the flexibility required for analyzing specific characteristics of RL methods often relevant to research. This paper presents Craftium, a novel framework for exploring and creating rich 3D visual RL environments that builds upon the Minetest game engine and the popular Gymnasium API. Minetest is built to be extended and can be used to easily create voxel-based 3D environments (often similar to Minecraft), while Gymnasium offers a simple and common interface for RL research. Craftium provides a platform that allows practitioners to create fully customized environments to suit their specific research requirements, ranging from simple visual tasks to infinite and procedurally generated worlds. We also provide five ready-to-use environments for benchmarking and as examples of how to develop new ones. The code and documentation are available at https://github.com/mikelma/craftium/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/2WL46CPN/Malagón et al. - 2024 - Craftium An Extensible Framework for Creating Reinforcement Learning Environments.pdf}
}

@misc{mouret2015,
  title = {Illuminating Search Spaces by Mapping Elites},
  author = {Mouret, Jean-Baptiste and Clune, Jeff},
  year = {2015},
  month = apr,
  number = {arXiv:1504.04909},
  eprint = {1504.04909},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-04-05},
  abstract = {Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Quantitative Biology - Populations and Evolution},
  file = {/Users/nobr/Zotero/storage/C4QL8U53/Mouret and Clune - 2015 - Illuminating search spaces by mapping elites.pdf;/Users/nobr/Zotero/storage/87F6KBH8/1504.html}
}

@article{ontanon2013,
  title = {The {{Combinatorial Multi-Armed Bandit Problem}} and {{Its Application}} to {{Real-Time Strategy Games}}},
  author = {Ontanon, Santiago},
  year = {2013},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  volume = {9},
  number = {1},
  pages = {58--64},
  issn = {2334-0924, 2326-909X},
  doi = {10.1609/aiide.v9i1.12681},
  urldate = {2024-01-14},
  abstract = {Game tree search in games with large branching factors is a notoriously hard problem. In this paper, we address this problem with a new sampling strategy for Monte Carlo Tree Search (MCTS) algorithms, called Na{\textasciidieresis}{\i}ve Sampling, based on a variant of the Multi-armed Bandit problem called the Combinatorial Multi-armed Bandit (CMAB) problem. We present a new MCTS algorithm based on Na{\textasciidieresis}{\i}ve Sampling called Na{\textasciidieresis}{\i}veMCTS, and evaluate it in the context of real-time strategy (RTS) games. Our results show that as the branching factor grows, Na{\i}{\textasciidieresis}veMCTS performs significantly better than other algorithms.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/6PUAMQYV/Ontanon - 2021 - The Combinatorial Multi-Armed Bandit Problem and Its Application to Real-Time Strategy Games.pdf}
}

@article{robbins1952,
  title = {{{SOME ASPECTS OF THE SEQUENTIAL DESIGN OF EXPERIMENTS}}},
  author = {Robbins, Herbert},
  year = {1952},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/UGMW65M7/Robbins - SOME ASPECTS OF THE SEQUENTIAL DESIGN OF EXPERIMENTS.pdf}
}

@misc{ruoss2024,
  title = {Grandmaster-{{Level Chess Without Search}}},
  author = {Ruoss, Anian and Del{\'e}tang, Gr{\'e}goire and Medapati, Sourabh and {Grau-Moya}, Jordi and Wenliang, Li Kevin and Catt, Elliot and Reid, John and Genewein, Tim},
  year = {2024},
  month = feb,
  number = {arXiv:2402.04494},
  eprint = {2402.04494},
  doi = {10.48550/arXiv.2402.04494},
  urldate = {2024-10-18},
  abstract = {The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess performance only arises at sufficient scale. To validate our results, we perform an extensive series of ablations of design choices and hyperparameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/WTWSFEBM/Ruoss et al. - 2024 - Grandmaster-Level Chess Without Search.pdf}
}

@article{shannon1950,
  type = {Bookitem},
  title = {Programming a {{Computer}} for {{Playing Chess}}},
  author = {Shannon, Claude E.},
  year = {1950},
  journal = {Computer Chess Compendium},
  pages = {2--13},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4757-1968-0_1},
  isbn = {9781475719703},
  langid = {english},
  keywords = {Chess Piece,Chess Player,General Purpose Computer,Human Player,Legal Move},
  file = {/Users/nobr/Zotero/storage/7ITCWRYU/2-0 and 2-1.Programming_a_computer_for_playing_chess.shannon.062303002.pdf}
}

@article{silver2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature 2016 529:7587},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/NATURE16961},
  urldate = {2023-03-22},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away. A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence. The victory in 1997 of the chess-playing computer Deep Blue in a six-game series against the then world champion Gary Kasparov was seen as a significant milestone in the development of artificial intelligence. An even greater challenge remained --- the ancient game of Go. Despite decades of refinement, until recently the strongest computers were still playing Go at the level of human amateurs. Enter AlphaGo. Developed by Google DeepMind, this program uses deep neural networks to mimic expert players, and further improves its performance by learning from games played against itself. AlphaGo has achieved a 99\% win rate against the strongest other Go programs, and defeated the reigning European champion Fan Hui 5--0 in a tournament match. This is the first time that a computer program has defeated a human professional player in even games, on a full, 19 x 19 board, in even games with no handicap.},
  isbn = {1051351651952},
  pmid = {26819042},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/nobr/Zotero/storage/DKVHJM6A/full-text.pdf}
}

@article{silver2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  urldate = {2024-01-13},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/nobr/Zotero/storage/WW8ABGUK/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf}
}

@article{silver2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2018},
  month = dec,
  journal = {Science},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aar6404},
  urldate = {2024-01-13},
  abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
  file = {/Users/nobr/Zotero/storage/YTYCIVNR/Silver et al. - 2018 - A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.pdf}
}

@article{stanley2002,
  title = {Evolving {{Neural Networks}} through {{Augmenting Topologies}}},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  year = {2002},
  month = jun,
  journal = {Evolutionary Computation},
  volume = {10},
  number = {2},
  pages = {99--127},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/106365602320169811},
  urldate = {2024-09-05},
  abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/557TL8NF/Stanley and Miikkulainen - 2002 - Evolving Neural Networks through Augmenting Topologies.pdf}
}

@article{tesau1995,
  title = {Temporal Difference Learning and {{TD-Gammon}}},
  author = {Tesau, Covid and Tesau, Gerald},
  year = {1995},
  month = mar,
  journal = {Communications of the ACM},
  volume = {38},
  number = {3},
  pages = {58--68},
  issn = {15577317},
  doi = {10.1145/203330.203343},
  urldate = {2023-03-21},
  abstract = {Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and ...},
  keywords = {/unread},
  annotation = {ACM\\
		PUB27\\
		New York, NY, USA},
  file = {/Users/nobr/Zotero/storage/ZKE3LV9B/full-text.pdf}
}

@misc{towers2024,
  title = {Gymnasium: {{A Standard Interface}} for {{Reinforcement Learning Environments}}},
  shorttitle = {Gymnasium},
  author = {Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U. and Cola, Gianluca De and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and {Perez-Vicente}, Rodrigo and Pierr{\'e}, Andrea and Schulhoff, Sander and Tai, Jun Jet and Tan, Hannah and Younis, Omar G.},
  year = {2024},
  month = nov,
  number = {arXiv:2407.17032},
  eprint = {2407.17032},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.17032},
  urldate = {2025-04-17},
  abstract = {Reinforcement Learning (RL) is a continuously growing field that has the potential to revolutionize many areas of artificial intelligence. However, despite its promise, RL research is often hindered by the lack of standardization in environment and algorithm implementations. This makes it difficult for researchers to compare and build upon each other's work, slowing down progress in the field. Gymnasium is an open-source library that provides a standard API for RL environments, aiming to tackle this issue. Gymnasium's main feature is a set of abstractions that allow for wide interoperability between environments and training algorithms, making it easier for researchers to develop and test RL algorithms. In addition, Gymnasium provides a collection of easy-to-use environments, tools for easily customizing environments, and tools to ensure the reproducibility and robustness of RL research. Through this unified framework, Gymnasium significantly streamlines the process of developing and testing RL algorithms, enabling researchers to focus more on innovation and less on implementation details. By providing a standardized platform for RL research, Gymnasium helps to drive forward the field of reinforcement learning and unlock its full potential. Gymnasium is available online at https://github.com/Farama-Foundation/Gymnasium},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Digital Libraries,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/LXFZGR37/Towers et al. - 2024 - Gymnasium A Standard Interface for Reinforcement Learning Environments.pdf}
}

@techreport{turing1950,
  title = {{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {Turing, A M},
  year = {1950},
  journal = {Computing Machinery and Intelligence. Mind},
  volume = {49},
  pages = {433--460},
  file = {/Users/nobr/Zotero/storage/RFBRNMV5/Turing - 1950 - COMPUTING MACHINERY AND INTELLIGENCE.pdf}
}
