@article{auerFinitetimeAnalysisMultiarmed2002,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicolo and Fischer, Paul},
  year = {2002},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2DIANKMA/Auer et al. - Finite-time Analysis of the Multiarmed Bandit Problem.pdf}
}

@article{browne2012,
  title = {A Survey of {{Monte Carlo}} Tree Search Methods},
  author = {Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  year = {2012},
  month = mar,
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  volume = {4},
  number = {1},
  pages = {1--43},
  issn = {1943068X},
  doi = {10.1109/TCIAIG.2012.2186810},
  urldate = {2023-03-22},
  abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work. {\copyright} 2009 IEEE.},
  keywords = {Artificial intelligence (AI),bandit-based methods,computer Go,game search,Monte Carlo tree search (MCTS),upper confidence bounds (UCB),upper confidence bounds for trees (UCT)},
  file = {/Users/nobr/Zotero/storage/JALIHS9D/Browne et al. - 2012 - A survey of Monte Carlo tree search methods.pdf}
}

@misc{kumarAutomatingSearchArtificial2024,
  title = {Automating the {{Search}} for {{Artificial Life}} with {{Foundation Models}}},
  author = {Kumar, Akarsh and Lu, Chris and Kirsch, Louis and Tang, Yujin and Stanley, Kenneth O. and Isola, Phillip and Ha, David},
  year = {2024},
  month = dec,
  number = {arXiv:2412.17799},
  eprint = {2412.17799},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.17799},
  urldate = {2025-01-09},
  abstract = {With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/NSHHFFYA/Kumar et al. - 2024 - Automating the Search for Artificial Life with Foundation Models.pdf}
}

@misc{mouretIlluminatingSearchSpaces2015,
  title = {Illuminating Search Spaces by Mapping Elites},
  author = {Mouret, Jean-Baptiste and Clune, Jeff},
  year = {2015},
  month = apr,
  number = {arXiv:1504.04909},
  eprint = {1504.04909},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-04-05},
  abstract = {Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Quantitative Biology - Populations and Evolution},
  file = {/Users/nobr/Zotero/storage/C4QL8U53/Mouret and Clune - 2015 - Illuminating search spaces by mapping elites.pdf;/Users/nobr/Zotero/storage/87F6KBH8/1504.html}
}

@article{ontanon2013,
  title = {The {{Combinatorial Multi-Armed Bandit Problem}} and {{Its Application}} to {{Real-Time Strategy Games}}},
  author = {Ontanon, Santiago},
  year = {2013},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  volume = {9},
  number = {1},
  pages = {58--64},
  issn = {2334-0924, 2326-909X},
  doi = {10.1609/aiide.v9i1.12681},
  urldate = {2024-01-14},
  abstract = {Game tree search in games with large branching factors is a notoriously hard problem. In this paper, we address this problem with a new sampling strategy for Monte Carlo Tree Search (MCTS) algorithms, called Na{\textasciidieresis}{\i}ve Sampling, based on a variant of the Multi-armed Bandit problem called the Combinatorial Multi-armed Bandit (CMAB) problem. We present a new MCTS algorithm based on Na{\textasciidieresis}{\i}ve Sampling called Na{\textasciidieresis}{\i}veMCTS, and evaluate it in the context of real-time strategy (RTS) games. Our results show that as the branching factor grows, Na{\i}{\textasciidieresis}veMCTS performs significantly better than other algorithms.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/6PUAMQYV/Ontanon - 2021 - The Combinatorial Multi-Armed Bandit Problem and Its Application to Real-Time Strategy Games.pdf}
}

@article{robbinsASPECTSSEQUENTIALDESIGN1952,
  title = {{{SOME ASPECTS OF THE SEQUENTIAL DESIGN OF EXPERIMENTS}}},
  author = {Robbins, Herbert},
  year = {1952},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/UGMW65M7/Robbins - SOME ASPECTS OF THE SEQUENTIAL DESIGN OF EXPERIMENTS.pdf}
}

@misc{ruossGrandmasterLevelChessSearch2024,
  title = {Grandmaster-{{Level Chess Without Search}}},
  author = {Ruoss, Anian and Del{\'e}tang, Gr{\'e}goire and Medapati, Sourabh and {Grau-Moya}, Jordi and Wenliang, Li Kevin and Catt, Elliot and Reid, John and Genewein, Tim},
  year = {2024},
  month = feb,
  number = {arXiv:2402.04494},
  eprint = {2402.04494},
  doi = {10.48550/arXiv.2402.04494},
  urldate = {2024-10-18},
  abstract = {The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess performance only arises at sufficient scale. To validate our results, we perform an extensive series of ablations of design choices and hyperparameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/WTWSFEBM/Ruoss et al. - 2024 - Grandmaster-Level Chess Without Search.pdf}
}

@article{shannon1950,
  type = {Bookitem},
  title = {Programming a {{Computer}} for {{Playing Chess}}},
  author = {Shannon, Claude E.},
  year = {1950},
  journal = {Computer Chess Compendium},
  pages = {2--13},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4757-1968-0_1},
  isbn = {9781475719703},
  langid = {english},
  keywords = {Chess Piece,Chess Player,General Purpose Computer,Human Player,Legal Move},
  file = {/Users/nobr/Zotero/storage/7ITCWRYU/2-0 and 2-1.Programming_a_computer_for_playing_chess.shannon.062303002.pdf}
}

@article{silver2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature 2016 529:7587},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/NATURE16961},
  urldate = {2023-03-22},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away. A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence. The victory in 1997 of the chess-playing computer Deep Blue in a six-game series against the then world champion Gary Kasparov was seen as a significant milestone in the development of artificial intelligence. An even greater challenge remained --- the ancient game of Go. Despite decades of refinement, until recently the strongest computers were still playing Go at the level of human amateurs. Enter AlphaGo. Developed by Google DeepMind, this program uses deep neural networks to mimic expert players, and further improves its performance by learning from games played against itself. AlphaGo has achieved a 99\% win rate against the strongest other Go programs, and defeated the reigning European champion Fan Hui 5--0 in a tournament match. This is the first time that a computer program has defeated a human professional player in even games, on a full, 19 x 19 board, in even games with no handicap.},
  isbn = {1051351651952},
  pmid = {26819042},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/nobr/Zotero/storage/DKVHJM6A/full-text.pdf}
}

@article{silver2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  urldate = {2024-01-13},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/nobr/Zotero/storage/WW8ABGUK/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf}
}

@article{silver2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2018},
  month = dec,
  journal = {Science},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aar6404},
  urldate = {2024-01-13},
  abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
  file = {/Users/nobr/Zotero/storage/YTYCIVNR/Silver et al. - 2018 - A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.pdf}
}

@article{stanleyEvolvingNeuralNetworks2002,
  title = {Evolving {{Neural Networks}} through {{Augmenting Topologies}}},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  year = {2002},
  month = jun,
  journal = {Evolutionary Computation},
  volume = {10},
  number = {2},
  pages = {99--127},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/106365602320169811},
  urldate = {2024-09-05},
  abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/557TL8NF/Stanley and Miikkulainen - 2002 - Evolving Neural Networks through Augmenting Topologies.pdf}
}

@article{tesau1995,
  title = {Temporal Difference Learning and {{TD-Gammon}}},
  author = {Tesau, Covid and Tesau, Gerald},
  year = {1995},
  month = mar,
  journal = {Communications of the ACM},
  volume = {38},
  number = {3},
  pages = {58--68},
  issn = {15577317},
  doi = {10.1145/203330.203343},
  urldate = {2023-03-21},
  abstract = {Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and ...},
  keywords = {/unread},
  annotation = {ACM\\
		PUB27\\
		New York, NY, USA},
  file = {/Users/nobr/Zotero/storage/ZKE3LV9B/full-text.pdf}
}

@techreport{turing1950,
  title = {{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {Turing, A M},
  year = {1950},
  journal = {Computing Machinery and Intelligence. Mind},
  volume = {49},
  pages = {433--460},
  file = {/Users/nobr/Zotero/storage/RFBRNMV5/Turing - 1950 - COMPUTING MACHINERY AND INTELLIGENCE.pdf}
}
